init:
input_size:
aux_ctc:

# normalize related
normalize: utterance_mvn
normalize_conf:
  norm_means: true
  norm_vars: false

# frontend related
frontend: default
frontend_conf:
  n_fft: 512
  win_length: 400
  hop_length: 160

# spec augment related
specaug: specaug
specaug_conf:
  apply_time_warp: true
  time_warp_window: 5
  time_warp_mode: bicubic
  apply_freq_mask: true
  freq_mask_width_range:
  - 0
  - 27
  num_freq_mask: 2
  apply_time_mask: true
  time_mask_width_ratio_range:
  - 0.
  - 0.05
  num_time_mask: 5

# encoder related
encoder: conformer
encoder_conf:
  output_size: 256
  attention_heads: 8
  linear_units: 2048
  num_blocks: 12
  dropout_rate: 0.1
  positional_dropout_rate: 0.1
  attention_dropout_rate: 0.1
  input_layer: "conv2d"
  normalize_before: true
  concat_after: false
  positionwise_layer_type: "linear"
  positionwise_conv_kernel_size: 3
  macaron_style: true
  rel_pos_type: "latest"
  pos_enc_layer_type: "rel_pos"
  selfattention_layer_type: "rel_selfattn"
  activation_type: "swish"
  use_cnn_module: true
  zero_triu: false
  cnn_module_kernel: 31
  padding_idx: -1
  interctc_layer_idx: [3,6,9]
  interctc_use_conditioning: true

# decoder related
decoder: mlm
decoder_conf:
  attention_heads: 8
  linear_units: 2048
  num_blocks: 6
  dropout_rate: 0.1
  positional_dropout_rate: 0.1
  self_attention_dropout_rate: 0.1
  src_attention_dropout_rate: 0.1

# ctc
ctc_conf:
  dropout_rate: 0.1
  ctc_type: "builtin"
  reduce: true

# model related
model: maskctc
model_conf:
  ctc_weight: 0.3
  interctc_weight: 0.5
  ignore_id: -1
  lsm_weight: 0.1
  length_normalized_loss: false
  report_cer: true
  report_wer: false
  sym_space: "‚ñÅ"
  sym_blank: "<blank>"
  aux_ctc:
    '3': lid_utt

# aux ctc tasks related
aux_ctc_tasks: ["lid_utt"]

# inference related
inference_conf:
  maskctc_n_iterations: 5
  maskctc_threshold_probability: 0.9
  device: "cpu"

# token related
token_type: bpe
bpemodel: "./src/tokenizers/spm/bbs-s2tc/256vocab/all_train_clean_lang.model"
token_list: "./src/tokenizers/spm/bbs-s2tc/256vocab/all_train_clean_lang.token"

# training related
training_settings:
  optimizer: "adam"
  scheduler: "noam"
  batch_size: 32
  warmup_steps: 10000
  learning_rate: 0.001
  noam_factor: 1.6
  accum_grad: 1
  grad_clip: -1.0
  epochs: 50
  average_epochs: 10
  use_amp: false
  num_workers: 8

dtype: "float32"
device: "cuda"
